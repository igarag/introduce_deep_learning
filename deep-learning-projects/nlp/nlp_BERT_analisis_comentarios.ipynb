{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_BERT_analisis_comentarios.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqb8oNYUllvU",
        "colab_type": "text"
      },
      "source": [
        "# Análisis de comentarios usando BERT.\n",
        "\n",
        "El objetivo de este cuadernillo es aterrizar y profundizar en el procesamiento del lenguaje natural (NLP) utilizando herramientas que se encuentran en el estado del arte, ver diferentes arquitecturas, etc, utilizando el mecanismo de atención que aportan los \"transformers\". Se utilizará [BERT](https://es.wikipedia.org/wiki/BERT_(sistema_computacional_de_comprensi%C3%B3n_de_lenguaje)) (Representación de Codificador Bidireccional de Transformadores) como modelo de propósito general de representación del lenguaje.\n",
        "\n",
        "- [Paper de Transformers](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "- [Paper de BERT](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "\n",
        "Este tutorial está basado en el vídeo de ['codificandobits'](https://www.youtube.com/watch?v=mvh7DV84mr4&list=PL9E7H1rzXKFL3a7LWs70jJ2qfpqRJ1E7h&index=5&t=1478s), agregando más información en aquellas partes donde necesito más apoyo para entender los fundamentos del NLP, Transformers y BERT. En este vídeo, se entrena con la base de datos de comentarios del sitio web dedicado al análisis de películas [IMdb](https://www.imdb.com/) para, una vez entrenado, probar con otro conjunto, uno que proviene de otro sitio web diferente (comentarios que nunca ha visto la red). Este tutorial toca muchas de las partes de la manera de trabajar de las redes neuronales en cuanto a análisis y preparación de datos, parámentros de entrenamiento y conclusiones finales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am89MucAt4xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkMFekcaun2m",
        "colab_type": "text"
      },
      "source": [
        "De la librería transformers se usará:\n",
        "\n",
        "- `BertModel`: Modelo preentrenado y descargado de BERT.\n",
        "- `BertTokenizer`: Herramienta de tokenización de BERT para convertir de palabras a valores numéricos.\n",
        "- `AdamW`: Optimizador.\n",
        "- `get_linear_schedule_with_warmup`: Se utiliza para hacer descender el valor de la tasa de aprendizaje a medida que se va entrenando."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZNvDx5ok4em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from textwrap import wrap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG_sScDyw6Y8",
        "colab_type": "text"
      },
      "source": [
        "## Montaje de Google Drive y acceso al dataset\n",
        "\n",
        "El *dataset* que se empleará es el de [\"Análisis de Sentimientos\" de Standford](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) que contiene 25.000 comentarios positivos y negativos para el entrenamiento y 25.000 para test.\n",
        "\n",
        "Montaremos la unidad de Google Drive para acceder al *dataset* que alojaremos en nuestra cuenta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUCHEECKNmMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc2eMO6Gw4oX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_PATH = '/content/drive/My Drive/datasets/sentiment_imdb_dataset.csv'\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "df = df[:10000]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s98_ZTT--Eyr",
        "colab_type": "text"
      },
      "source": [
        "## Inicialización de parámetros\n",
        "\n",
        "Se inicializa la semilla a un valor constante para conseguir los mismos resultados que en el tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zqjf0Cb7Dgq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Semilla de inicialización fija.\n",
        "RANDOM_SEED = 42\n",
        "MAX_LEN = 200\n",
        "BATCH_SIZE = 16\n",
        "NCLASSES = 2\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "# Selección de GPU:\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwFes9dDN4LL",
        "colab_type": "text"
      },
      "source": [
        "## Exploración y preparación de los datos\n",
        "\n",
        "### Cambio de nombre a la columna \"sentimiento\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUvl8s8xN6bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cambiamos la columna \"sentimiento\" por \"etiqueta\"\n",
        "df.columns = [\"review\", \"label\"]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvKwVYk8OHFH",
        "colab_type": "text"
      },
      "source": [
        "### Transformación de la etiqueta a valor numérico\n",
        "\n",
        "- `positive` --> 1\n",
        "- `negative` --> 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dls6bn3NOVag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(DATASET_PATH) # pendiente de borrar\n",
        "df = df[:10000] # pendiente de borrar\n",
        "\n",
        "df.columns = [\"review\", \"label\"]\n",
        "df['label'] = (df['label']=='positive').astype(int)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFhaloDCRNIZ",
        "colab_type": "text"
      },
      "source": [
        "## Construcción de una estructura válida para BERT ([CLS] y [SEP]) y tokenización\n",
        "\n",
        "### Carga del modelo preentrenado de BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jIxF4jQRTnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQaNtK0JUneE",
        "colab_type": "text"
      },
      "source": [
        "Ejemplo de tokenización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KVoA9w1UiqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_txt = 'I really loved that movie!'\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print('Frase: ', sample_txt)\n",
        "print('Tokens: ', tokens)\n",
        "print('Tokens numéricos: ', token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAg5Nm1PU-LQ",
        "colab_type": "text"
      },
      "source": [
        "Codificación par introducir el dato a BERT. Esto requiere introducir los caracteres especiales `[CLS]` y `[SEP]`. Para ello se utiliza el codificador `plus`. Introduciendo una frase, el tokenizador colocará automáticamente los caracteres especiales. **Retorna un diccionario con dos claves: los `input_ids` y la `atention_mask`**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlmrJ85OVQqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "    sample_txt,\n",
        "    max_length = 10,  # Longitud máxima del texto\n",
        "    truncation = True,  # Corta el texto hasta cumplir el argumento anterior\n",
        "    add_special_tokens = True,  # Añade estos caracteres [cls], [sep] y los caracteres vacíos\n",
        "    return_token_type_ids = False,  \n",
        "    pad_to_max_length = True,  # Relleno con el token de padding\n",
        "    return_attention_mask = True,  # Durante el entrenamiento se presetará atención únicamente a la parte != 0\n",
        "    return_tensors = 'pt'  # Convierte la frase en un contenido ya numérico para introducir a BERT.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhVn4WQCWkDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMcyQO4wWulR",
        "colab_type": "text"
      },
      "source": [
        "Podemos usar la función inversa que convierte de valores numéricos a palabras. Eso está dentro de la clase `tokenizer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F06SWoVLW3kW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Tokenizador: {tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])}\")\n",
        "print(f\"ids: {encoding['input_ids'][0]}\")\n",
        "print(f\"Attention mask: {encoding['attention_mask'][0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3t6wBVkaSWu",
        "colab_type": "text"
      },
      "source": [
        "## Creación del Dataset\n",
        "\n",
        "Dividimos el dataset de 10000 en entrenamiento y validación (8000, 2000). Pero estos 8000 datos no se pueden entregar de golpe a la red, por varios motivos: limitación de memoria RAM....aleatoriedad de las muestras. Por tanto se presenta en \"trozos\" o *batches*. El tamaño de este *batch* se fijará en 16 (frases). Cuando presente todos los datos (16*500 = 8000) habrá completado una época.\n",
        "\n",
        "Para todo esto se crea una clase que hará todo esto. Heredamos de esta clase, la clase `Dataset` de Pytorch, que nos ayuda a esta partición del dataset. Esta clase será llamada por otra función: DataLoader (la que realmente lee los datos)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBq6G_0Lbq1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IMDBDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, labels, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    '''\n",
        "    Esta es la función de PyTorch va leyendo para obtener esos batches de 16 \n",
        "    aleatoriamente.\n",
        "    '''\n",
        "    review = str(self.reviews[item])  # Texto en bruto (RAW)\n",
        "    label = self.labels[item]\n",
        "    # Aquí, aún tenemos el texto en bruto. Vamos a tokenizarlo y agregarle \n",
        "    # los caracteres especiales\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        review,\n",
        "        max_length = self.max_len,\n",
        "        truncation = True,\n",
        "        add_special_tokens = True,\n",
        "        return_token_type_ids = False,  \n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "    return {\n",
        "        'review': review,\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'label': torch.tensor(label, dtype=torch.long) # <-- lo pasamos a tensor\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJFcYHbBXARW",
        "colab_type": "text"
      },
      "source": [
        "Construimos ahora el `DataLoader` (que pytorch requiere para leer del dataset)\n",
        "\n",
        "Convertirmos los datos (la columna review del dataset) en formato Numpy (para que pueda ser entendido por la red) y lo mismo con las etiquetas. El tokenizador ya lo tenemos calculado de antes, al igual que el tamaño máximo. Construimos el cargador de datos de pytorch con el argumento `num_workers` para que pueda procesar de manera paralela los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBrtSoQDf4Xo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_loader(df, tokenizer, max_len, batch_size):\n",
        "\n",
        "  dataset = IMDBDataset(\n",
        "      reviews=df.review.to_numpy(),\n",
        "      labels=df.label.to_numpy(),\n",
        "      tokenizer=tokenizer,\n",
        "      max_len=MAX_LEN\n",
        "  )\n",
        "  return DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHIL8VHMgz5N",
        "colab_type": "text"
      },
      "source": [
        "Dividimos el dataser utilizando la función de `sklearn: train_test_split` dividiendo en 20% (0.2): 80% train y 20% de validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxxaRTOCgvOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_O4YDKPj5Cb",
        "colab_type": "text"
      },
      "source": [
        "## Creación del modelo de acoplamiento para el ajuste del modelo de BERT a nuestro problema (*transfer learning*).\n",
        "\n",
        "Creación de una clase para el modelo y entrenamiento.\n",
        "\n",
        "`nn`: Neural Network\n",
        "\n",
        "Primero se define la estructura y luego cómo se conectan las capas entre sí.\n",
        "\n",
        "Lo que haremos es: agregar al modelo BERT (preentrenado) una red neuronal (con una única capa) con nuestro clasificador de sentimientos.\n",
        "\n",
        "Atención con esta línea:\n",
        "\n",
        "`self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)`\n",
        "\n",
        "Estamos diciendo el número de neuronas de entrada y de salida. El primer argumento será la salida (el nº de neuronas) de BERT (768 neuronas de salida) y `n_classes` (2 clases, positivo o negativo).\n",
        "\n",
        "*Nótese que podríamos haber puesto el literal `768` como el número de salidas de la capa de BERT, pero, por buenas prácticas, se extrae esa información de los parámetros del modelo ya entrenado.*\n",
        "\n",
        "Para reducir el *overfitting* agregamos una capa adicional intermedia entre las capas de `Dropout` (descarte de conexiones de un 30% en cada época)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jhYirvMii4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSentimentClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self, n_classes):\n",
        "    super(BERTSentimentClassifier, self).__init__()  # Instanciamos la clase superior\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME) # carga del modelo pre-entrenado\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes) # Capa lineal de neuronas\n",
        "\n",
        "  def forward(self, input_ids, attention_mask): # datos de entrada del modelo de BERT\n",
        "    '''\n",
        "    BERT retorna dos datos (si recordamos de lo que vimos arriba) pero solo \n",
        "    nos interesa la codificación del token de clasificación, ignorando la \n",
        "    primera salida.\n",
        "    '''\n",
        "    _, cls_output = self.bert(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask = attention_mask\n",
        "    )\n",
        "    drop_output = self.drop(cls_output)\n",
        "    final_output = self.linear(drop_output)\n",
        "\n",
        "    return final_output # aquí tenemos la clase de salida, positivo o negativo\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Cr2jVyVT3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BERTSentimentClassifier(NCLASSES)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7XBz_NyZhXQ",
        "colab_type": "text"
      },
      "source": [
        "Podemos extraer la información del modelo imprimiendolo viendo como al final de este se encuentra la conexión entre la salida de BERT y la entrada a nuestro clasificador (pasando por el dropout).\n",
        "\n",
        "```\n",
        "BERT\n",
        ". . . \n",
        "\n",
        "(drop): Dropout(p=0.3, inplace=False)\n",
        "(linear): Linear(in_features=768, out_features=2, bias=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_evKjBnZ_cF",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Esta sección en PyTorch es un poco más compleja que en Keras pero lo vemos paso a paso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fkwNnscZ6nN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCH = 5\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCH # Este será el número de veces TOTALES que se tiene que hacer iteraciones de entrenamiento\n",
        "scheduler = get_linear_schedule_with_warmup( # La tasa de aprendizaje irá disminuyendo en cada Epoca\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFK636CayJD",
        "colab_type": "text"
      },
      "source": [
        "## Creación del código para el entrenamiento con una iteración de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAqwTm44arbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "  losses = [] # Almacenamiento del valor del error\n",
        "  correct_predictions = 0\n",
        "  for batch in data_loader: # Cogerá un batch de 16\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds==labels)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Para evitar que el gradiente crezca demasiado\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses) # <-- valor promedio del error\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples): # evaluar modelo\n",
        "  model = model.eval() # se congela el modelo (evitamos que cambien los pesos)\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds==labels)\n",
        "\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqeEFe_QcfM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCH):\n",
        "  print(f'Epoch: {epoch+1} / {EPOCH}')\n",
        "  train_acc, train_loss = train_model(\n",
        "      model,\n",
        "      train_data_loader,\n",
        "      loss_fn,\n",
        "      optimizer,\n",
        "      device,\n",
        "      scheduler,\n",
        "      len(df_train)\n",
        "  )\n",
        "  test_acc, test_loss = eval_model(\n",
        "      model,\n",
        "      test_data_loader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(df_test)\n",
        "  )\n",
        "  print(f\"Train loss: {train_loss}, accuracy: {train_acc}\")\n",
        "  print(f\"Test loss: {test_loss}, accuracy: {test_acc}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTFoFrtv3e6P",
        "colab_type": "text"
      },
      "source": [
        "## Guardando el model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhxG38BW3dxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/models/setimental_analysis_using_BERT.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkk3sDUdkhtE",
        "colab_type": "text"
      },
      "source": [
        "## Evaluación del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOUpy9mPkjoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify_film_comment(review_text):\n",
        "  encoding_review = tokenizer.encode_plus(\n",
        "      review_text,\n",
        "      max_length=MAX_LEN,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  input_ids = encoding_review['input_ids'].to(device)\n",
        "  attention_mask = encoding_review['attention_mask'].to(device)\n",
        "\n",
        "  output = model(input_ids, attention_mask)\n",
        "\n",
        "  _, pred = torch.max(output, dim=1)\n",
        "\n",
        "  print(f'Input : {wrap(review_text)}')\n",
        "\n",
        "  if pred:\n",
        "    print(f'Output: POSITIVO')\n",
        "  else:\n",
        "    print(f'Output: NEGATIVO')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDQO5vcplpOU",
        "colab_type": "text"
      },
      "source": [
        "### Ejemplo para la evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxMzb4k_loVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_text = \"Avengers: Infinity War at least had the good taste to abstain from Jeremy Renner. No such luck in Endgame.\"\n",
        "\n",
        "classify_film_comment(review_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}