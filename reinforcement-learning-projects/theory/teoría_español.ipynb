{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook pretende ser una guía de iniciación a los conceptos del Aprendizaje por Refuerzo. La información es extraida de los libros:\n",
    "\n",
    "- [Reinforcement Learning: An Introduction (Richard S. Sutton and Andrew G. Barto)](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).\n",
    "- [Deep Reinforcement Learning - Hands-On (Maxim Lapam)](https://subscription.packtpub.com/book/data/9781838826994).\n",
    "\n",
    "El aprendizaje por refuerzo (RL) es una rama del *Machine Learning* en la que un **agente**, o conjunto de agentes (MultiAgent Systems), realiza **observaciones** dentro de un **entorno**, para ejecutar una serie de **acciones** y obtener una **recompensa** en función de la acción elegida. Un ejemplo puede ser:\n",
    "\n",
    "- **Entorno**: Laberinto\n",
    "- **Agente**: Humano\n",
    "- **Observaciones**: Vista\n",
    "- **Acciones**: Alante / Atrás / Izquierda / Derecha\n",
    "- **Recompensa**: Salir del laberinto.\n",
    "\n",
    "Al **principio** el agente será desconocedor absoluto del entorno y realizará las acciones de forma **aleatoria**. Con el paso del tiempo las acciones le llevarán a alcanzar el objetivo que previamente hemos definido. Este proceso se conoce como el dilema **exploración/explotación**. Inicialmente, el agente no conoce en que entorno se encuentra por lo que **explora** sus opciones con acciones aleatorias. Poco a poco, esas pequeñas acciones comienzan a devolverle recompensas mayores hasta llegar al objetivo final. En este proceso de \"cambio\" de un estado a otro, el agente utilizará todo lo aprendido para **explotar** el conjunto de acciones que le llevan a la recompensa máxima.\n",
    "\n",
    "En el ejemplo de un laberinto, inicialmente no sabemos el camino. Es a base de distintos intentos (al principio fallidos) donde el agente se va aprendiendo (exploración) el camino hasta dar con la salida. Una vez consigue la salida, posteriores intentos solucionarán el laberinto de manera inmediata, dado que ya se conoce el camino (explotación).\n",
    "\n",
    "Este equilibrio hace que el agente aprenda más sobre el entorno y tome mejores decisiones ante nuevas observaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesos de Decisión de Markov\n",
    "\n",
    "Otro conjunto de términos y definiciones en el RL son: **estado, episodio, historia y valor**. Estos conceptos se entienden dentro de los procesos de decisión de Markov (*Markov Decision Processes*).\n",
    "\n",
    "### Cadena de Markov.\n",
    "\n",
    "El elemento más básico de la familia de los procesos de Markov es **la cadena de Markov**. Una cadena de Markov es una secuencia de eventos que se dan con cierta probabilidad y que está compuesta por **estados** y **transiciones**. Esta *cadenas* de estados puede ejemplificarse con un modelo simple: el tiempo. Una secuencia de observaciones durante el tiempo crea una cadena de estados como: [soleado, nublado, nublado, soleado, ...] que generan una **historia**. El conjunto de estados se denomina **espacio de estados** (state space).\n",
    "\n",
    "Este ejemplo es muy simple dado que parece que no guarda relación el tiempo que hace hoy con el que hizo ayer y realmente entran en juego varios factores que no se han tenido en cuenta (época del año, latitud, etc) pero ejemplifica la idea inicial de este concepto. Extendiendo un poco más la cadena de Markov están las **matrices de transición** que muestran la probabilidad de cambiar a uno u otro estado y define la dinámica del sistema. Puede verse un ejemplo en la siguiente tabla:\n",
    "\n",
    "|        |soleado|lluvioso|\n",
    "|------- |-------|--------|\n",
    "|soleado |  0.8  |  0.2   |\n",
    "|lluvioso|  0.1  |  0.9   |\n",
    "\n",
    "Si juntamos los conceptos de cadena con la matriz de transición se tiene como resultado la siguiente imagen:\n",
    "\n",
    "![](./images/markov_chain.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".virtualenvs",
   "language": "python",
   "name": ".virtualenvs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
